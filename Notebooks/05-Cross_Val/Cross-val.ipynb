{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, sys # For filepaths\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Add the datasets and libraries to the system path '''\n",
    "\n",
    "# Find the path to our implementations\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "home_directory = os.path.dirname(parent_directory)\n",
    "libraries_path = os.path.join(home_directory, 'Libraries')\n",
    "\n",
    "# Find the path to the datasets\n",
    "datasets_path = os.path.join(home_directory, 'Datasets')\n",
    "\n",
    "# Add them both to the system path\n",
    "sys.path.append(datasets_path)\n",
    "sys.path.append(libraries_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (10000, 785)\n"
     ]
    }
   ],
   "source": [
    "''' Load the dataset '''\n",
    "\n",
    "dataset = np.load(os.path.join(datasets_path, 'fashion_train.npy'))\n",
    "print(\"Dataset shape: \", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:, :-1]  # Define the images as all columns except the last one\n",
    "y = dataset[:, -1] # Define the labels as the last column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from Ania's FYP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import classifiers \n",
    "from sklearn import svm \n",
    "#from CNN import CNN\n",
    "#from Bayes import Bayes_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Recall = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_recall[i]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage ROC AUC = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_roc_auc[i]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m \u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifiers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m, in \u001b[0;36mget_metrics\u001b[1;34m(classifiers, x, y)\u001b[0m\n\u001b[0;32m     21\u001b[0m predictions \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(x_val)\n\u001b[0;32m     22\u001b[0m acc_val[i, j] \u001b[38;5;241m=\u001b[39m accuracy_score(y_val, predictions)\n\u001b[1;32m---> 23\u001b[0m f1_val[i, j] \u001b[38;5;241m=\u001b[39m \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m precision[i, j] \u001b[38;5;241m=\u001b[39m precision_score(y_val, predictions, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     25\u001b[0m recall[i, j] \u001b[38;5;241m=\u001b[39m recall_score(y_val, predictions)\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1146\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf1_score\u001b[39m(\n\u001b[0;32m   1012\u001b[0m     y_true,\n\u001b[0;32m   1013\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1020\u001b[0m ):\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1287\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfbeta_score\u001b[39m(\n\u001b[0;32m   1159\u001b[0m     y_true,\n\u001b[0;32m   1160\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1167\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1168\u001b[0m ):\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \n\u001b[0;32m   1171\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;124;03m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1391\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1389\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1390\u001b[0m             average_options\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1392\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m but average=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1393\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoose another average setting, one of \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (y_type, average_options)\n\u001b[0;32m   1394\u001b[0m         )\n\u001b[0;32m   1395\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pos_label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1396\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1397\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that pos_label (set to \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) is ignored when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage != \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m). You may use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1402\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = [svm.SVC()]\n",
    "classifier_names = [\"SVM\"]\n",
    "num_classifiers = len(classifiers)\n",
    "num_folds = 5\n",
    "\n",
    "group_kfold = KFold(n_splits=num_folds)\n",
    "\n",
    "# Define functions to avoid code repetition\n",
    "def get_metrics(classifiers, x, y):\n",
    "    acc_val = np.empty([num_folds, num_classifiers])\n",
    "    f1_val = np.empty([num_folds, num_classifiers])\n",
    "    precision = np.empty([num_folds, num_classifiers])\n",
    "    recall = np.empty([num_folds, num_classifiers])\n",
    "    roc_auc = np.empty([num_folds, num_classifiers])\n",
    "    for i, (train_index, val_index) in enumerate(group_kfold.split(x, y)):\n",
    "        x_train, x_val = x[train_index, :], x[val_index, :]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        for j, clf in enumerate(classifiers): \n",
    "            clf.fit(x_train, y_train)\n",
    "            predictions = clf.predict(x_val)\n",
    "            acc_val[i, j] = accuracy_score(y_val, predictions)\n",
    "            f1_val[i, j] = f1_score(y_val, predictions)\n",
    "            precision[i, j] = precision_score(y_val, predictions, zero_division=0)\n",
    "            recall[i, j] = recall_score(y_val, predictions)\n",
    "            roc_auc[i, j] = roc_auc_score(y_val, predictions)\n",
    "    print_metrics(acc_val, f1_val, precision, recall, roc_auc)\n",
    "    return x_train, y_train, x_val, y_val  # return these for later use in ROC plot\n",
    "\n",
    "def print_metrics(acc_val, f1_val, precision, recall, roc_auc):\n",
    "    average_acc = np.mean(acc_val, axis=0) \n",
    "    average_f1 = np.mean(f1_val, axis=0)\n",
    "    average_precision = np.mean(precision, axis=0)\n",
    "    average_recall = np.mean(recall, axis=0)\n",
    "    average_roc_auc = np.mean(roc_auc, axis=0)\n",
    "    for i, classifier_name in enumerate(classifier_names):\n",
    "        print(f\"############ Classifier {i+1} - {classifier_name}:\")\n",
    "        print(f'Average F1 score = {average_f1[i]:.3f}')\n",
    "        print(f'Average Accuracy = {average_acc[i]:.3f}')\n",
    "        print(f'Average Precision = {average_precision[i]:.3f}')\n",
    "        print(f'Average Recall = {average_recall[i]:.3f}')\n",
    "        print(f'Average ROC AUC = {average_roc_auc[i]:.3f}')\n",
    "\n",
    "get_metrics(classifiers, X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal feature selection\n",
    "print(\"Running without feature selection\")\n",
    "x_train, y_train, x_val, y_val = get_metrics(classifiers, x, y, patient_id)\n",
    "\n",
    "# Feature selection with variance threshold\n",
    "print(\"Running with variance threshold feature selection\")\n",
    "threshold_value = 0.1\n",
    "selector = VarianceThreshold(threshold=threshold_value)\n",
    "x_selected = selector.fit_transform(x)\n",
    "get_metrics(classifiers, x_selected, y, patient_id, \"With variance threshold\")\n",
    "\n",
    "# Feature selection with PCA\n",
    "print(\"Running with PCA feature selection\")\n",
    "pca_transformer = PCA(n_components=5)\n",
    "x_pca = pca_transformer.fit_transform(x)\n",
    "get_metrics(classifiers, x_pca, y, patient_id, \"With PCA\")\n",
    "\n",
    "# Define the path where to save the file\n",
    "pickle_path = 'Project-2_github_repo/Fixed/PROJECT/fyp2023/model_group02'\n",
    "\n",
    "# Chosen classifier\n",
    "classifier = KNN(n_neighbors=5)\n",
    "classifier = classifier.fit(x, y)\n",
    "filename = 'group02_classifier.sav'\n",
    "\n",
    "# Check if the path exists, if not, create it\n",
    "if not os.path.exists(pickle_path):\n",
    "    os.makedirs(pickle_path)\n",
    "\n",
    "pickle.dump(classifier, open(os.path.join(pickle_path, filename), 'wb'))\n",
    "\n",
    "\n",
    "# Plot ROC curve for best classifier\n",
    "clf = classifier\n",
    "clf.fit(x_train, y_train)\n",
    "y_score = clf.predict_proba(x_val)\n",
    "fpr, tpr, _ = roc_curve(y_val, y_score[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
